{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'title', 'title_en', 'title_id', 'url', 'description',\n",
      "       'date_published', 'category', 'sub_category', 'aspect',\n",
      "       'sentiment_label', 'sentiment_confidence'],\n",
      "      dtype='object')\n",
      "(281538, 12)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "pathName = '../../data2/split-dataset/translate_labeling/'\n",
    "fileNames = os.listdir(pathName)\n",
    "df_list = []\n",
    "for fileName in fileNames:\n",
    "    if fileName.endswith(\".csv\"):  \n",
    "        dataset = pd.read_csv(f'{pathName}{fileName}')\n",
    "        df_list.append(dataset)\n",
    "dataset = pd.concat(df_list, ignore_index=True)\n",
    "print(dataset.columns)\n",
    "print(dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281332"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-processing\n",
    "dataset = dataset.dropna(subset=[\"title\"])\n",
    "dataset = dataset.dropna(subset=[\"sentiment_label\"])\n",
    "dataset = dataset[(dataset.date_published < '2022-03-31 23:59:59')\n",
    "                  & (dataset.date_published > '2017-01-01 00:00:00')]\n",
    "title = []\n",
    "for index, row in dataset.iterrows():\n",
    "    # print(row['title'])\n",
    "    row['title'] = row['title'].lower().replace('&nbsp; ', '').strip()\n",
    "    row['title'] = re.sub(r\"\\(\\d+\\/\\d+\\)\", \"\", row['title'])\n",
    "    title.append(row['title'])\n",
    "dataset['title'] = title\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting train - test dataset\n",
    "X_train_aspect, X_test_aspect, y_train_aspect, y_test_aspect = train_test_split(dataset['title'],dataset['aspect'],stratify=dataset['aspect'],test_size=0.20)\n",
    "X_train_sentiment, X_test_sentiment, y_train_sentiment, y_test_sentiment = train_test_split(dataset['title'],dataset['sentiment_label'],stratify=dataset['sentiment_label'],test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset text-gcn\n",
    "df_aspect_train = pd.DataFrame(list(zip(X_train_aspect, y_train_aspect)), columns=['title', 'aspect'])\n",
    "df_aspect_train['type'] = 'train'\n",
    "df_aspect_test = pd.DataFrame(list(zip(X_test_aspect, y_test_aspect)), columns=['title', 'aspect'])\n",
    "df_aspect_test['type'] = 'test'\n",
    "\n",
    "df_sentimen_train = pd.DataFrame(list(zip(X_train_sentiment, y_train_sentiment)), columns=['title', 'sentiment'])\n",
    "df_sentimen_train['type'] = 'train'\n",
    "df_sentimen_test = pd.DataFrame(list(zip(X_test_sentiment, y_test_sentiment)), columns=['title', 'sentiment'])\n",
    "df_sentimen_test['type'] = 'test'\n",
    "\n",
    "df_aspect = pd.concat([df_aspect_train,df_aspect_test],ignore_index=True)\n",
    "df_sentiment = pd.concat([df_sentimen_train,df_sentimen_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_dict = {'industri': 0, 'internasional': 1,\n",
    "                'investasi': 2, 'keuangan': 3, 'nasional': 4}\n",
    "sentiment_dict = {'NEGATIVE': 0, 'POSITIVE': 1}\n",
    "df_aspect['aspect_code'] = df_aspect.aspect.map(aspect_dict)\n",
    "df_sentiment['sentiment_code'] = df_sentiment.sentiment.map(sentiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358, 1)\n"
     ]
    }
   ],
   "source": [
    "# load stop words\n",
    "stopwords = pd.read_csv('../../data2/stopwords_indonesia.csv')\n",
    "print(stopwords.shape)\n",
    "\n",
    "def preprocessing(text, stopwords):\n",
    "    # Removing punctuations like . , ! $( ) * % @\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Lower casing\n",
    "    text = text.lower()\n",
    "    # Filter just alphabet\n",
    "    text = re.sub(r\"[^a-z\\s]+\", \"\", text)\n",
    "    # Tokenization\n",
    "    token = nltk.word_tokenize(text)\n",
    "    # Removing Stop words\n",
    "    token = [i for i in token if i not in stopwords]\n",
    "    # Removing Single char\n",
    "    token = [i for i in token if len(i) > 1]\n",
    "    # Convert number\n",
    "    return token\n",
    "\n",
    "def dataset_processing(dataset):\n",
    "    token_sentence = {'token':[],'sentence':[]}\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        token = preprocessing(row['title'], stopwords['list'].tolist())\n",
    "        token_sentence['token'].append(token)\n",
    "        token_sentence['sentence'].append(' '.join(token))\n",
    "    \n",
    "    dataset['token'] = token_sentence['token']\n",
    "    dataset['token_title'] = token_sentence['sentence']\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aspect = dataset_processing(df_aspect)\n",
    "df_sentiment = dataset_processing(df_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aspect[['type','aspect_code']].to_csv(f'data/text_dataset/kontan1.txt', sep ='\\t', header=False)\n",
    "df_aspect['token_title'].to_csv(f'data/text_dataset/corpus/kontan1.txt', sep ='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment[['type','sentiment_code']].to_csv(f'data/text_dataset/kontan2.txt', sep ='\\t', header=False)\n",
    "df_sentiment['token_title'].to_csv(f'data/text_dataset/corpus/kontan2.txt', sep ='\\t', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d235b7b7af7208e894e92181a9a5bdb0d890feca452994c5abfd4c6fc97dc2d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
